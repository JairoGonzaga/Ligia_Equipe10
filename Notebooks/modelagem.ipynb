{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Setup Inicial**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Carregamento do projeto e dados**\n",
    "\n",
    "- Importa bibliotecas principais\n",
    "- Clona o repositório e adiciona o src ao sys.path\n",
    "- Faz o pré-processamento dos dados com prepare_data()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, recall_score, f1_score, confusion_matrix, roc_auc_score, accuracy_score, roc_curve, precision_recall_curve, average_precision_score, ConfusionMatrixDisplay\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.preprocessing import prepare_data\n",
    "X_train, X_test, y_train, y_test, preprocessor = prepare_data(\"../Data/heart.csv\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Importação dos modelos e variações dos seus hiperparâmetros**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Baseline:** Antes de comparar vários modelos diferentes, é importante definir um baseline simples.  \n",
    "Escolhemos utilizar **Logistic Regression** como baseline, principalmente porque:\n",
    "\n",
    "- é um modelo linear simples;\n",
    "- treina rápido;\n",
    "- serve como referência mínima para saber se modelos mais complexos realmente valem a pena;\n",
    "- é bastante usado em problemas de classificação binária na área da saúde.\n",
    "\n",
    "**A partir daqui, todos os outros modelos serão comparados diretamente com o desempenho dele.**\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##**Baseline (Modelo de Referência)**\n",
    "\n",
    "- Cria um pipeline com pré-processamento + LogisticRegression\n",
    "- Treina no treino e avalia no teste\n",
    "- Calcula métricas para servir de referência (baseline)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "print(\"Treinando baseline (Logistic Regression)...\")\n",
    "\n",
    "baseline_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=300))\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "baseline_preds = baseline_pipeline.predict(X_test)\n",
    "baseline_probs = baseline_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "baseline_results = {\n",
    "    \"modelo\": \"Baseline (LogReg)\",\n",
    "    \"accuracy\": accuracy_score(y_test, baseline_preds),\n",
    "    \"recall\": recall_score(y_test, baseline_preds),\n",
    "    \"f1\": f1_score(y_test, baseline_preds),\n",
    "    \"auc\": roc_auc_score(y_test, baseline_probs)\n",
    "}\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definição dos modelos e hiperparâmetros\n",
    "\n",
    "- Importa modelos candidatos\n",
    "- Define o espaço de busca de hiperparâmetros (RandomizedSearchCV)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importação de modelos\n",
    "!pip install xgboost\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Definindo range de Hiperparametros\n",
    "models_config = [\n",
    "    {\n",
    "        'name': 'RandomForest',\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 300, 500],        # N° de árvores\n",
    "            'model__max_depth': [None, 10, 20],            # Profundidade\n",
    "            'model__class_weight': ['balanced', None]      # Pesos\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'SVM',\n",
    "        'model': SVC(probability=True, random_state=42),\n",
    "        'params': {\n",
    "            'model__C': [0.1, 1, 10, 100],                 # Margem de erro\n",
    "            'model__kernel': ['linear', 'rbf'],            # Tipo de linha\n",
    "            'model__class_weight': ['balanced', None]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'XGBoost',\n",
    "        'model': XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        'params': {\n",
    "            'model__n_estimators': [100, 300],             # Árvores de reforço\n",
    "            'model__learning_rate': [0.01, 0.1, 0.2],      # Velocidade de aprendizado\n",
    "            'model__max_depth': [3, 5, 7],                 # Complexidade de cada árvore\n",
    "            # XGBoost usa 'scale_pos_weight' para balancear classes.\n",
    "            # Como não sabemos o valor exato agora, vamos testar alguns multiplicadores\n",
    "            'model__scale_pos_weight': [1, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Rede Neural (MLP)',\n",
    "        'model': MLPClassifier(max_iter=1000, random_state=42), # max_iter alto para dar tempo de aprender\n",
    "        'params': {\n",
    "            'model__hidden_layer_sizes': [(50,), (100,), (50, 50)], # Camadas e neurônios\n",
    "            'model__activation': ['relu', 'tanh'],\n",
    "            'model__alpha': [0.0001, 0.05],                         # Regularização\n",
    "            'model__learning_rate': ['constant', 'adaptive']        # Velocidade de aprendizado\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'KNN',\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'model__n_neighbors': [3, 5, 7, 9],  # N° de vizinhos\n",
    "            'model__weights': ['uniform', 'distance'],\n",
    "            'model__metric': ['euclidean', 'manhattan'] # Medição de distância\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **RandomizedSearchCV**\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Busca de hiperparâmetros com RandomizedSearchCV**\n",
    "\n",
    "- Para cada modelo, cria um Pipeline (pré-processador + modelo)\n",
    "- Executa busca aleatória com validação cruzada\n",
    "- Armazena o melhor recall e os melhores parâmetros"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = []\n",
    "print (\"Inciando RandomizedSearchCV\")\n",
    "for config in models_config:\n",
    "    print(f\"\\nTreinando: {config['name']}...\")\n",
    "\n",
    "    # Cria um pipeline específico só para ess modelo\n",
    "    pipe = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', config['model']) # Evita Data Leakage\n",
    "        ])\n",
    "\n",
    "    # Config. da Busca Aleatória\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=config['params'],\n",
    "        n_iter=15,             # Testa 15 combinações por modelo\n",
    "        scoring='recall',      # O critério de vitória é o Recall!\n",
    "        cv=5,                  # Validação cruzada de 5 dobras\n",
    "        random_state=42,\n",
    "        n_jobs=-1              # Usa todo o poder do processador\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Salva o melhor resultado do modelo\n",
    "    results.append({\n",
    "        'Modelo': config['name'],\n",
    "        'Melhor Recall': search.best_score_,\n",
    "        'Melhores Params': search.best_params_,\n",
    "        'Melhor Modelo Treinado': search.best_estimator_\n",
    "    })\n",
    "\n",
    "    print(f\"  -> Melhor Recall do {config['name']}: {search.best_score_:.4f}\")\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Comparação: Baseline vs todos os modelos (usando os melhores resultados)**\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tabela comparativa dos resultados\n",
    "\n",
    "- Consolida baseline e resultados da busca em um DataFrame\n",
    "- Ordena por Recall para facilitar comparação"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rows = [baseline_results.copy()]\n",
    "\n",
    "for item in results:\n",
    "    nome = item[\"Modelo\"]\n",
    "    modelo = item[\"Melhor Modelo Treinado\"]\n",
    "\n",
    "    preds = modelo.predict(X_test)\n",
    "\n",
    "    if hasattr(modelo, \"predict_proba\"):\n",
    "        probs = modelo.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"modelo\": nome,\n",
    "        \"accuracy\": accuracy_score(y_test, preds),\n",
    "        \"recall\": recall_score(y_test, preds),\n",
    "        \"f1\": f1_score(y_test, preds),\n",
    "        \"auc\": auc\n",
    "    })\n",
    "\n",
    "df_all = pd.DataFrame(rows).sort_values(by=\"recall\", ascending=False).reset_index(drop=True)\n",
    "df_all"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "No gráfico observado acima, em que a comparação feita entre modelos de acordo com os escores de accuracy, recall, f1 e auc. Esse escoers são métricas de validação utilizadas para analisar o rendimento do modelo, no qual se destacando 3 dos 5 modleos analisados pelo grupo (KNN, LorReg, RandomForest), despertando assim a atenção do frupo para possiveís modelos de escolha para a solução do problema/desafio do grupo."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Gráficos (modelos vs baseline)\n",
    "\n",
    "Cada gráfico mostra o desempenho dos modelos e a linha que representa o baseline. Essas visualizações ajudam a comparar diretamente como cada modelo se comporta em relação à referência inicial.\n",
    "\n",
    "**Comparação final e ranking das métricas**\n",
    "\n",
    "- Normaliza as métricas em relação ao baseline\n",
    "- Gera uma tabela final ordenada por Recall\n",
    "- Seleciona o melhor modelo global\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = [\"recall\", \"f1\", \"accuracy\", \"auc\"]\n",
    "\n",
    "baseline_row = df_all[df_all[\"modelo\"] == \"Baseline (LogReg)\"].iloc[0]\n",
    "df_plot = df_all[df_all[\"modelo\"] != \"Baseline (LogReg)\"].copy()\n",
    "\n",
    "x = np.arange(len(df_plot[\"modelo\"]))\n",
    "\n",
    "for m in metrics:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(x, df_plot[m].values)\n",
    "    plt.axhline(baseline_row[m], linestyle=\"--\", linewidth=2,\n",
    "                label=f\"Baseline = {baseline_row[m]:.3f}\")\n",
    "    plt.title(f\"{m.upper()} — modelos vs baseline\")\n",
    "    plt.ylabel(m)\n",
    "    plt.xticks(x, df_plot[\"modelo\"], rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Os gráficos mostram métricas de avaliação dos modelos (XGBos, Random Forest, KNN, SVM, MLP), comparando-os utilizando uma baseline como cutoff onde os escores (Recall, Accuracy, F1 e AUC dos modelos vs baseline) comparam o desempenho de cinco modelos em um dataset desbalanceado de risco cardíaco.\n",
    "\n",
    "Recall: Mede a fração de casos reais de risco de infarto detectados pelos modelos (sensibilidade); baseline em 0.612. Todos os modelos superam ligeiramente a baseline (~0.65-0.75), com variações pequenas; bom para priorizar detecção de positivos.\n",
    "​\n",
    "\n",
    "Accuracy: Proporção geral de previsões corretas; baseline em 0.886. Modelos atingem ~0.89-0.90, com Adaboost e RF próximos ou acima; útil em classes balanceadas, mas menos aqui devido ao desbalanceamento.\n",
    "​\n",
    "​\n",
    "\n",
    "F1 Score: Média harmônica de precisão e recall, balanceando falsos positivos/negativos; baseline em 0.899. Modelos ~0.90-0.91, ideais para datasets desbalanceados como este.\n",
    "​\n",
    "​\n",
    "\n",
    "AUC (ROC): Área sob a curva ROC, medindo discriminação entre classes em vários thresholds; baseline em 0.933. Modelos ~0.94-0.95, indicando excelente capacidade preditiva independentemente do corte."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Métrica principal: Recall**\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A métrica principal escolhida foi **Recall**, porque o objetivo do projeto é reduzir ao máximo os **falsos negativos**.\n",
    "\n",
    "Neste problema, um falso negativo significa classificar como “baixo risco” um paciente que na verdade tem risco de infarto.  \n",
    "Esse tipo de erro é o mais crítico, porque pode atrasar um atendimento, um exame complementar ou uma intervenção preventiva.\n",
    "\n",
    "O recall mede exatamente isso: **entre os pacientes que realmente são positivos (risco real), quantos o modelo conseguiu identificar**.  \n",
    "Por isso, entre modelos com desempenho parecido, priorizamos o que apresenta **maior recall**.\n",
    "\n",
    "As outras métricas (accuracy, f1 e AUC) ainda ajudam a complementar a análise, mas o critério central de escolha do melhor modelo é o recall."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dados\n",
    "baseline_recall = baseline_results[\"recall\"]\n",
    "df_recall = df_all[df_all[\"modelo\"] != \"Baseline (LogReg)\"][[\"modelo\", \"recall\"]].copy()\n",
    "\n",
    "# ordena por recall (pra ficar bem evidente)\n",
    "df_recall = df_recall.sort_values(by=\"recall\", ascending=True).reset_index(drop=True)\n",
    "\n",
    "x = np.arange(len(df_recall[\"modelo\"]))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(x, df_recall[\"recall\"].values)\n",
    "plt.axhline(baseline_recall, linestyle=\"--\", linewidth=2, label=f\"Baseline = {baseline_recall:.3f}\")\n",
    "\n",
    "plt.title(\"Recall — modelos vs baseline\")\n",
    "plt.ylabel(\"recall\")\n",
    "plt.xticks(x, df_recall[\"modelo\"], rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# só pra ficar explícito no output também\n",
    "melhor = df_recall.iloc[-1]\n",
    "print(f\"Maior recall: {melhor['modelo']} ({melhor['recall']:.4f})\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Após termos feito a escolha dao Recall, foi analisado novamento o rendimento dos modelos para analisar qual deles tiveram a maior Recall. Assim vemos que o XGBoost passou do baseline de 0,912 com um escore de 0.9706."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Verficações de Segurança**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Função para checar overfitting (treino vs teste)**\n",
    "\n",
    "- Compara desempenho do modelo no treino e no teste\n",
    "- Ajuda detectar sinais de overfitting (diferença grande entre métricas)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def verificar_overfitting(modelo_treinado, X_teste, y_teste, recall_validacao):\n",
    "    previsao_teste = modelo_treinado.predict(X_teste)\n",
    "    recall_teste = recall_score(y_teste, previsao_teste)\n",
    "\n",
    "    # Diferença entre o treino e teste\n",
    "    diferenca = recall_validacao - recall_teste\n",
    "\n",
    "    print(f\"Recall na Validação (Estudo): {recall_validacao:.2%}\")\n",
    "    print(f\"Recall no Teste (Prova Real): {recall_teste:.2%}\")\n",
    "\n",
    "    if diferenca > 0.10: # Se cair mais de 10%\n",
    "        print(f\"Overfitting! O modelo caiu {diferenca:.1%} no teste.\")\n",
    "    elif diferenca < -0.05:\n",
    "        print(f\"O modelo foi muito melhor no teste. Verifique os dados.\")\n",
    "    else:\n",
    "        print(f\"SUCESSO! Modelo Sólido. A performance se manteve.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Cáculo Top 3 melhores**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Ordenação e escolha do melhor modelo**\n",
    "\n",
    "- Ordena a lista de resultados por Recall\n",
    "- Seleciona o melhor estimador treinado\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Ordena a lista de resultados pelo Recall (do maior para o menor)\n",
    "results_sorted = sorted(results, key=lambda x: x['Melhor Recall'], reverse=True)\n",
    "\n",
    "print(\"xX TOP 3 MODELOS COM MELHOR RECALL Xx\")\n",
    "for i, res in enumerate(results_sorted[:3]):\n",
    "    print(f\"\\n#{i+1} LUGAR: {res['Modelo']}\")\n",
    "    print(f\"   Recall Médio (Validação): {res['Melhor Recall']:.4f}\")\n",
    "    print(f\"   Configuração Vencedora: {res['Melhores Params']}\")\n",
    "\n",
    "# Seleciona o campeão absoluto para usar no teste final\n",
    "best_global_model = results_sorted[0]['Melhor Modelo Treinado']"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Após analisarmos os resultados do Recall, foi feito uma análise dos tres modelos que se destacaram, entre eles (SVM, RandomForest e XGBoost), sendo o XGBoost o modelo vencedor tendo o maior escore do Recall médio (validação) = 0.9827."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Analisando o melhor modelo**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Define variáveis auxiliares e prepara dados para gráficos de avaliação"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"--- VERIFICAÇÃO DE ESTABILIDADE (OVERFITTING) ---\\n\")\n",
    "\n",
    "# Verificamos novamente,porém só o melhor dessa vez\n",
    "best_model = results_sorted[0]\n",
    "print(f\"Analisando: {best_model['Modelo']}\")\n",
    "\n",
    "# Chama a função criada acima mais uma vez só por segurança\n",
    "verificar_overfitting(\n",
    "    modelo_treinado=best_model['Melhor Modelo Treinado'],\n",
    "    X_teste=X_test,\n",
    "    y_teste=y_test,\n",
    "    recall_validacao=best_model['Melhor Recall']\n",
    ")\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Matriz de Confusão\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Gera e plota matriz de confusão no conjunto de teste"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pegamos o melhor modelo\n",
    "campeao = best_global_model\n",
    "\n",
    "# Geramos a matriz\n",
    "print(f\"Visualizando performance do melhor modelo: {results_sorted[0]['Modelo']}\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    campeao,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=['Saudável', 'Risco de Infarto'],\n",
    "    cmap='Blues',\n",
    "    values_format='d',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.title(\"Onde o Modelo Acertou e Errou\")\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Esta matriz de confusão mostra como o nosso modelo está lidando com o nosso dataset utilizado para fazer as predições do risco ou não de infarto. Ao analisar como o modelo fez as predições vemos que a precisão da classe 0 (saudável) vemos um leve imprecisão, onde classificou 38 das 82, dando um resultado falso positivo (Risco de Infarto), no entanto esse má classificação não causa tanto \"problema\" pelo fato dos pacientes estarem saudáveis. Entretanto, no classe 1 (Risco de Infarto) vemos o contrário, onde o nosso modleo conseguiu classificou muito bem tendo so 3 pacientes dos 102, classificados como saudáveis (classe 0) sendo na verdade paciente com Risco de Infarto. Mas mesmo que esse valores causam certo problema, a precisão em geral de classificar a classe 1 está na faixa de 0.9705, aceitando essa imprecisão na margem de erro do nosso modelo."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gráfico ROC e AUC"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Calcula probabilidades do modelo\n",
    "- Plota ROC e PR para avaliar trade-off de limiar"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_probs = best_global_model.predict_proba(X_test)[:, 1] # Pega as probabilidades da classe positiva\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, y_probs) # Cálculo o ROC\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, y_probs)\n",
    "avg_precision = average_precision_score(y_test, y_probs)\n",
    "\n",
    "# Desenho do gráfico\n",
    "ax1.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.3f})')\n",
    "ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Linha do \"chute\" (50%)\n",
    "ax1.set_title('Capacidade de Separação (ROC)')\n",
    "ax1.set_xlabel('Alarmes Falsos (Falsos Positivos)')\n",
    "ax1.set_ylabel('Acertos Reais (Recall)')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(recall, precision, color='green', lw=2, label=f'Precision-Recall (AP = {avg_precision:.3f})')\n",
    "ax2.set_title('Precisão vs. Recall')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precisão')\n",
    "ax2.legend(loc=\"lower left\")\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Esse gráfico apresenta a Curva ROC (Receiver Operating Characteristic) e a Curva Precision-Recall de um modelo preditivo, provavelmente para detecção de risco de infarto, junto com suas respectivas áreas sob a curva (AUC e AUC-PR). A Curva ROC plota taxa de verdadeiros positivos (sensibilidade/recall) contra falsos positivos (1-especificidade), avaliando a capacidade de discriminação em diferentes thresholds.\n",
    "​\n",
    "\n",
    "Curva ROC e AUC\n",
    "A curva laranja mostra o desempenho do modelo, próxima à diagonal superior esquerda, indicando alta capacidade de separar classes (risco vs. sem risco). A linha azul diagonal representa desempenho aleatório (AUC=0.5).\n",
    "​\n",
    "​\n",
    "\n",
    "AUC-ROC ≈ 0.95 (estimado visualmente, alinhado com gráficos prévios ~0.93-0.96), próximo de 1 (excelente), significando que o modelo distingue bem pacientes com risco de infarto.\n",
    "\n",
    "Quanto maior a área sob a curva laranja, melhor; aqui, supera muito o aleatório.\n",
    "\n",
    "Curva Precision-Recall e AUC-PR\n",
    "A curva verde plota precisão (proporção correta entre preditos positivos) contra recall, útil em datasets desbalanceados como saúde (mais saudáveis que riscos). Começa alta em precisão e cai gradualmente.\n",
    "​\n",
    "​\n",
    "\n",
    "AUC-PR ≈ 0.95 (ótimo), refletindo bom equilíbrio mesmo com poucos casos positivos; superior em cenários onde falsos positivos custam caro (ex.: exames desnecessários).\n",
    "\n",
    "Threshold ótimo equilibra as duas, evitando overdetection.\n",
    "\n",
    "Essas curvas confirmam o modelo como robusto para predição de doenças cardíacas, consistente com baselines e outros gráficos compartilhados."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Considerações sobre o limiar de decisão\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Calcula F1 para diferentes limiares\n",
    "- Escolhe limiar que maximiza a métrica desejada\n",
    "- Mostra impacto do novo limiar"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mesmo que o limiar otimizado tenha aumentado a precisão e a acurácia,  \n",
    "optamos por manter o limiar padrão (0.50). O motivo é que o objetivo principal  \n",
    "é reduzir falsos negativos — ou seja, evitar que um paciente com risco real  \n",
    "seja classificado como seguro.\n",
    "\n",
    "Como o limiar otimizado reduz levemente a sensibilidade para positivos,  \n",
    "a decisão final foi manter o limiar original, que favorece o maior número  \n",
    "possível de detecções corretas entre os casos realmente positivos."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pega as probalidades calculadas acima\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "# Procura o Limiar ideal\n",
    "ix = np.argmax(fscore)\n",
    "best_thresh = thresholds_pr[ix]\n",
    "\n",
    "print(\"xX RESULTADO DA OTIMIZAÇÃO Xx\")\n",
    "print(f\"Limiar Padrão: 0.5000\")\n",
    "print(f\"Melhor Limiar Encontrado: {best_thresh:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Neste novo ponto, o desempenho seria:\")\n",
    "print(f\"-> Recall:    {recall[ix]:.4f} (Deixamos passar poucos)\")\n",
    "print(f\"-> Precisão:  {precision[ix]:.4f} (Erramos menos alarmes)\")\n",
    "\n",
    "# Teste de acurácia da mudança\n",
    "novas_previsoes = (y_probs >= best_thresh).astype(int)\n",
    "\n",
    "acc_antiga = accuracy_score(y_test, best_global_model.predict(X_test))\n",
    "acc_nova = accuracy_score(y_test, novas_previsoes)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Acurácia Original (Limiar 0.5): {acc_antiga:.2%}\")\n",
    "print(f\"Acurácia Otimizada (Limiar {best_thresh:.2f}): {acc_nova:.2%}\")\n",
    "\n",
    "if acc_nova > acc_antiga:\n",
    "    print(\"Apesar do aumento na acurácia e na precisão com o novo limiar,\")\n",
    "    print(\"optamos por manter o limiar padrão. Nosso foco é evitar falsos negativos,\")\n",
    "    print(\"então priorizamos o ponto que garante o maior número possível de acertos\")\n",
    "    print(\"para pacientes que realmente têm risco.\")\n",
    "else:\n",
    "    print(\"O limiar padrão (0.5) permanece como a opção mais equilibrada para o modelo.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Transparência do Modelo**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Importância de features (XGBoost)**\n",
    "\n",
    "- Extrai nomes das features após one-hot/transformações\n",
    "- Obtém feature_importances_ do XGBoost\n",
    "- Plota Top-10 features mais relevantes"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "modelo_xgboost = best_global_model.named_steps['model'] # Pega do pipeline a \"inteligência da IA\"\n",
    "\n",
    "# Pega os dados brutos, antes do pré-processamento, usando o preprocessor treinado do melhor modelo\n",
    "nomes_colunas = best_global_model.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "# Ranking de Sintomas\n",
    "importancias = modelo_xgboost.feature_importances_\n",
    "\n",
    "# Gráfico\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_sintomas = pd.DataFrame({'Sintoma': nomes_colunas, 'Peso': importancias})\n",
    "df_top10 = df_sintomas.sort_values(by='Peso', ascending=False).head(10)\n",
    "\n",
    "plt.barh(df_top10['Sintoma'], df_top10['Peso'], color='red')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('O que o modelo considera Risco de Infarto?')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Importância de Features\n",
    "\n",
    "Gráfico de barras horizontais mostra o que o modelo (provavelmente XGBoost) usa mais para prever risco: \"cat_ST_Slope\" domina (~0.35), seguido de \"cat_ChestPainType\", \"cat_ExerciseAngina\", etc. Features categóricas de ECG, dor torácica e angina são cruciais; ajuda interpretar o \"black box\" do modelo."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusão do Modelo Final\n",
    "\n",
    "O modelo escolhido foi o **XGBoost**, pois apresentou o **maior Recall** entre todos os modelos avaliados. Essa métrica foi definida como principal porque, no contexto de risco de infarto, o erro mais crítico é o falso negativo — quando um paciente de risco é classificado como seguro. O recall mede justamente quantos desses casos o modelo é capaz de identificar, e por isso é o critério central da escolha.\n",
    "\n",
    "Além do recall superior, o XGBoost também mostrou bom desempenho em f1-score, AUC e estabilidade entre treino e teste, indicando que generaliza bem e não apresentou overfitting relevante. O limiar otimizado é apenas um ajuste opcional da forma como o modelo transforma probabilidades em decisões, mas não altera o modelo final escolhido.\n",
    "\n",
    "Assim, o **XGBoost** é o modelo mais adequado para o problema, equilibrando desempenho e segurança clínica.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
